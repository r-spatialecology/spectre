---
title: "Example run of OurFOAM"
output: html_notebook
---

This document uses the functions built in the package to develop an estimate of the species composition for one landscape generated by the EFFoRTS-ABM.

First, there are quite a few libraries we need, primarily to run everything in parallel.

```{r message=FALSE, warning=FALSE, include=FALSE}
packages <- c("OurFOAM", "iNEXT", "bigmemory", "foreach", "doParallel", "tidyverse")
lapply(packages, library, character.only = TRUE)
devtools::load_all(".")
```

Next we load all the data we need to get started, i.e. the maps of the predictors, the table of environmental variables, and the gdm model.

```{r}
## Import the landscape rasters generated by Netlogo
maps <- OurFOAM::load_raster("Data/", "*.asc")

# Import the environmental data frame and filter out the jungle values
env <- read.table("Data/EnvironmentalData_Mark2_reduced.txt", header=TRUE)
env$PlotID <- substr(env$PlotID, 2, 2)
env <- env %>% filter(PlotID != "J")

## Import the GDM built for the birds data
birds_gdm <- readRDS("Data/Birds_gdm.rds")
```

Next to make the code run in a useable amount of time while sorting everything out lets massively reduce the size of the maps and thus the area we are trying to predict. Starting with a 10 x 10

```{r}
maps <- crop(maps, extent(maps, 41, 50, 41, 50))
dim(maps)
```

Now we need to generate the data actually used by the DynamicFOAM algorithm. First, we generate a table of simulated environmental data.

```{r}
## Prepare lutmap (replace numbers with strings and convert to data frame)
lutmap <- maps[[4]]
lutmap[lutmap==4] <- "F"
lutmap[lutmap==2] <- "R"
lutmap[lutmap==1] <- "O"
lutdf <- as.data.frame(lutmap, xy=TRUE)
names(lutdf) <- c("x", "y", "lut")

## Generate environmental table:
envdat <- simulate_environmental_data(lutdf, env)
```

Next we create an estimate of the alpha diversity in the landscape

```{r}
bird_alpha <- round(exp(2.078083 + 0.027132 * maps[[2]] + 
                          0.010402 * maps[[1]] - 
                          0.476163 * maps[[5]] + 
                          2.201850 * maps[[3]]))

alpha_list <- as.vector(as.matrix(bird_alpha))
```

Then the beta diversity in a massive pair-wise matrix. This is a bit of a bottleneck, so we run it in parallel, using big memory to ensure it doesn't crash. This code is a cow to get running correctly but luckily we only need to run it once then we can import the big matrix later on. So I am just including it here for completeness.

```{r eval=FALSE}
# ## 3) Beta diversity / dissimilarity
# ##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##
# ## This is a bit of a bottleneck, so we run it in parallel, using big memory to 
# ## ensure it doesn't crash
# 
# ## Detect cores:
# no_cores <- 2
# 
# ## Generate a list with combinations of rows and cols we want to fill (only upper matrix and diagonal)
# m <- matrix(ncol = nrow(envdat), nrow=nrow(envdat))
# comb <- expand.grid(row = 1:nrow(m), col = 1:ncol(m)) # grid
# comb <- comb[lower.tri(m, diag = FALSE), ]
# rm(m)
# 
# rows_per_chunk <- nrow(comb) / no_cores
# comb$chunk <- rep(1:no_cores, each=rows_per_chunk)
# 
# ## Create big data matrix:
# predictions <- big.matrix(nrow=nrow(envdat), 
#                           ncol=nrow(envdat), 
#                           type="double", 
#                           init=NA, 
#                           backingfile = "bigmatrix.mat", 
#                           descriptorfile = "bigmatrix.desc")
# 
# ## Create parallel cluster:
# cl <- makeCluster(no_cores, outfile="cluster.log")
# registerDoParallel(cl)
# 
# ## Execute:
# t.start <- Sys.time()
# foreach(i = 1:no_cores, .packages = c("bigmemory", "dplyr", "gdm", "OurFOAM", "devtools")) %dopar%
#   {
#     devtools::load_all("F:/PostDoc/Gottingen/EFForTS/Focus3/Package/dynamicfoam")
#     t <- attach.big.matrix("bigmatrix.desc")
#     comb_chunk <- subset(comb, comb$chunk == i)
#     
#     for (j in 1:nrow(comb_chunk))
#     {
#       ind_row <- comb_chunk[j,]$row
#       ind_col <- comb_chunk[j,]$col
#       
#       t[ind_row, ind_col] <- OurFOAM::predict_pair_dissimilarity(ind_row, ind_col, envdat, birds_gdm)
#       
#       # progress <- data.frame(node=i, proc=(j / nrow(comb_chunk) * 100))
#       # if (progress$proc %% 0.1 == 0)
#       # {
#       #   write.table(progress, file="Outputs/progress.log", sep="\t", row.names=F, col.names=F, append=TRUE)
#       # }
#     }
#     NULL
#   }
# stopCluster(cl)
# t.end <- Sys.time()
# t.end - t.start
```


Last we need to estimate the total gamma diversity i.e. total number of species in the landscape.

```{r message=FALSE, warning=FALSE}
bird_species <- read_csv2("Data/B09_Birds_plot.csv")
total_recorded_species <- rowSums(bird_species[-1])
estimated_species <- iNEXT::iNEXT(total_recorded_species, q=0, datatype="abundance")
estimated_gamma <- round(estimated_species$AsyEst[1,2]) #139 for this example
```


---
Now we calculate the target pairwise commonness matrix. We use the previously created big matrix of pairwise beta for this

```{r}
bmo <- attach.big.matrix("bigmatrix.desc")

target_matrix <- calculate_commonness(alpha_list, bmo)
```

---
Lastly we run the massive optimization algorithm. This is not quick, even for this tiny landscape.

```{r}
t.start <- Sys.time()

species_grid <- run_optimization(alpha_list, estimated_gamma, target_matrix, 1500, 1000, 500)

t.end <- Sys.time()
t.end - t.start
```

